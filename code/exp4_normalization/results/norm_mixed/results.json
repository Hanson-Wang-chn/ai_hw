{
  "config": {
    "experiment": {
      "name": "norm_mixed",
      "description": "混合归一化：Encoder用Pre-LN，Decoder用Post-LN"
    },
    "model": {
      "d_model": 512,
      "nhead": 8,
      "num_encoder_layers": 3,
      "num_decoder_layers": 3,
      "dim_feedforward": 2048,
      "dropout": 0.1,
      "activation": "relu",
      "pe_type": "sinusoidal",
      "norm_type": "mixed",
      "encoder_norm_type": "pre",
      "decoder_norm_type": "post",
      "attn_type": "multi_head"
    },
    "training": {
      "lr": 0.0001,
      "batch_size": 32,
      "epochs": 50,
      "patience": 5,
      "lr_patience": 3,
      "lr_factor": 0.5,
      "grad_clip": 1.0,
      "label_smoothing": 0.1,
      "weight_decay": 0.01
    },
    "data": {
      "max_seq_len": 20,
      "min_freq": 2
    }
  },
  "test_metrics": {
    "bleu_1": 0.02468094082902647,
    "bleu_2": 0.008018668483230665,
    "bleu_4": 0.004774925052524541,
    "rouge_l": 0.06117487462422324,
    "bert_score": 0.4818650186061859,
    "loss": 0.5686923153698444
  },
  "loss_variance": 0.2604459962014937,
  "convergence_epoch": 1,
  "train_loss_history": [
    2.818561314349681,
    1.6977858774166192,
    1.5179748966366844,
    1.436606625012592,
    1.391989139592753,
    1.3620420216986564
  ],
  "val_loss_history": [
    0.5643172599375248,
    0.3033977560698986,
    0.21507144253700972,
    0.1751575123053044,
    0.15034346864558756,
    0.14213602081872523
  ],
  "bleu_1": 0.02468094082902647,
  "bleu_2": 0.008018668483230665,
  "bleu_4": 0.004774925052524541,
  "rouge_l": 0.06117487462422324,
  "bert_score": 0.4818650186061859,
  "loss": 0.5686923153698444
}