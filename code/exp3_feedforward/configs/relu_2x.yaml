# 实验三：ReLU + 2倍隐藏层（轻量化）
experiment:
  name: "ffn_relu_2x"
  description: "ReLU激活 + 2倍隐藏层维度（轻量化设计，调整层数保持参数量一致）"

model:
  d_model: 512
  nhead: 8
  num_encoder_layers: 4  # 增加层数以补偿参数量
  num_decoder_layers: 4
  dim_feedforward: 1024  # 2 * d_model
  dropout: 0.1
  activation: "relu"
  pe_type: "sinusoidal"
  norm_type: "post"
  attn_type: "multi_head"

training:
  lr: 0.0001
  batch_size: 32
  epochs: 50
  patience: 5
  lr_patience: 3
  lr_factor: 0.5
  grad_clip: 1.0
  label_smoothing: 0.1
  weight_decay: 0.01

data:
  max_seq_len: 20
  min_freq: 2
