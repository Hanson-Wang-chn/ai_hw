{
  "attention_bidirectional": {
    "config": {
      "experiment": {
        "name": "attention_bidirectional",
        "description": "双向注意力：Encoder端双向自注意力，Decoder端单向+交叉注意力"
      },
      "model": {
        "d_model": 512,
        "nhead": 8,
        "num_encoder_layers": 3,
        "num_decoder_layers": 3,
        "dim_feedforward": 2048,
        "dropout": 0.1,
        "activation": "relu",
        "pe_type": "sinusoidal",
        "norm_type": "post",
        "attn_type": "bidirectional"
      },
      "training": {
        "lr": 0.0001,
        "batch_size": 32,
        "epochs": 50,
        "patience": 5,
        "lr_patience": 3,
        "lr_factor": 0.5,
        "grad_clip": 1.0,
        "label_smoothing": 0.1,
        "weight_decay": 0.01
      },
      "data": {
        "max_seq_len": 20,
        "min_freq": 2
      }
    },
    "test_metrics": {
      "bleu_1": 0.02468094082902647,
      "bleu_2": 0.008018668483230665,
      "bleu_4": 0.004774925052524541,
      "rouge_l": 0.06117487462422324,
      "bert_score": 0.48186495900154114,
      "loss": 0.5714231915771961
    },
    "training_time": 184.14520001411438,
    "attention_entropy": 2.6447547912597655,
    "bleu_1": 0.02468094082902647,
    "bleu_2": 0.008018668483230665,
    "bleu_4": 0.004774925052524541,
    "rouge_l": 0.06117487462422324,
    "bert_score": 0.48186495900154114,
    "loss": 0.5714231915771961
  },
  "attention_linear": {
    "config": {
      "experiment": {
        "name": "attention_linear",
        "description": "Linear Attention (Choromanski et al., 2020)，减少计算量"
      },
      "model": {
        "d_model": 512,
        "nhead": 8,
        "num_encoder_layers": 3,
        "num_decoder_layers": 3,
        "dim_feedforward": 2048,
        "dropout": 0.1,
        "activation": "relu",
        "pe_type": "sinusoidal",
        "norm_type": "post",
        "attn_type": "linear"
      },
      "training": {
        "lr": 0.0001,
        "batch_size": 32,
        "epochs": 50,
        "patience": 5,
        "lr_patience": 3,
        "lr_factor": 0.5,
        "grad_clip": 1.0,
        "label_smoothing": 0.1,
        "weight_decay": 0.01
      },
      "data": {
        "max_seq_len": 20,
        "min_freq": 2
      }
    },
    "test_metrics": {
      "bleu_1": 0.006539394578630945,
      "bleu_2": 0.0021246044699158163,
      "bleu_4": 0.0012651510822928272,
      "rouge_l": 0.010244966542522783,
      "bert_score": 0.5458039045333862,
      "loss": 0.17302867746911943
    },
    "training_time": 293.6425178050995,
    "attention_entropy": 2.644728994369507,
    "bleu_1": 0.006539394578630945,
    "bleu_2": 0.0021246044699158163,
    "bleu_4": 0.0012651510822928272,
    "rouge_l": 0.010244966542522783,
    "bert_score": 0.5458039045333862,
    "loss": 0.17302867746911943
  },
  "attention_scaled_dot": {
    "config": {
      "experiment": {
        "name": "attention_scaled_dot",
        "description": "标准Scaled Dot-Product Attention (基准组)"
      },
      "model": {
        "d_model": 512,
        "nhead": 1,
        "num_encoder_layers": 3,
        "num_decoder_layers": 3,
        "dim_feedforward": 2048,
        "dropout": 0.1,
        "activation": "relu",
        "pe_type": "sinusoidal",
        "norm_type": "post",
        "attn_type": "scaled_dot"
      },
      "training": {
        "lr": 0.0001,
        "batch_size": 32,
        "epochs": 50,
        "patience": 5,
        "lr_patience": 3,
        "lr_factor": 0.5,
        "grad_clip": 1.0,
        "label_smoothing": 0.1,
        "weight_decay": 0.01
      },
      "data": {
        "max_seq_len": 20,
        "min_freq": 2
      }
    },
    "test_metrics": {
      "bleu_1": 0.02468094082902647,
      "bleu_2": 0.008018668483230665,
      "bleu_4": 0.004774925052524541,
      "rouge_l": 0.06117487462422324,
      "bert_score": 0.48186495900154114,
      "loss": 0.5918847545981407
    },
    "training_time": 183.60607886314392,
    "attention_entropy": 2.644754934310913,
    "bleu_1": 0.02468094082902647,
    "bleu_2": 0.008018668483230665,
    "bleu_4": 0.004774925052524541,
    "rouge_l": 0.06117487462422324,
    "bert_score": 0.48186495900154114,
    "loss": 0.5918847545981407
  },
  "attention_multi_head": {
    "config": {
      "experiment": {
        "name": "attention_multi_head",
        "description": "Multi-Head Attention (8头)"
      },
      "model": {
        "d_model": 512,
        "nhead": 8,
        "num_encoder_layers": 3,
        "num_decoder_layers": 3,
        "dim_feedforward": 2048,
        "dropout": 0.1,
        "activation": "relu",
        "pe_type": "sinusoidal",
        "norm_type": "post",
        "attn_type": "multi_head"
      },
      "training": {
        "lr": 0.0001,
        "batch_size": 32,
        "epochs": 50,
        "patience": 5,
        "lr_patience": 3,
        "lr_factor": 0.5,
        "grad_clip": 1.0,
        "label_smoothing": 0.1,
        "weight_decay": 0.01
      },
      "data": {
        "max_seq_len": 20,
        "min_freq": 2
      }
    },
    "test_metrics": {
      "bleu_1": 0.02468094082902647,
      "bleu_2": 0.008018668483230665,
      "bleu_4": 0.004774925052524541,
      "rouge_l": 0.06117487462422324,
      "bert_score": 0.48186495900154114,
      "loss": 0.5735621964558959
    },
    "training_time": 190.19368052482605,
    "attention_entropy": 2.6447561264038084,
    "bleu_1": 0.02468094082902647,
    "bleu_2": 0.008018668483230665,
    "bleu_4": 0.004774925052524541,
    "rouge_l": 0.06117487462422324,
    "bert_score": 0.48186495900154114,
    "loss": 0.5735621964558959
  }
}