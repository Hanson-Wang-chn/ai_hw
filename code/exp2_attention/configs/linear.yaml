# 实验二：线性注意力配置
experiment:
  name: "attention_linear"
  description: "Linear Attention (Choromanski et al., 2020)，减少计算量"

model:
  d_model: 512
  nhead: 8
  num_encoder_layers: 3
  num_decoder_layers: 3
  dim_feedforward: 2048
  dropout: 0.1
  activation: "relu"
  pe_type: "sinusoidal"
  norm_type: "post"
  attn_type: "linear"

training:
  lr: 0.0001
  batch_size: 32
  epochs: 50
  patience: 5
  lr_patience: 3
  lr_factor: 0.5
  grad_clip: 1.0
  label_smoothing: 0.1
  weight_decay: 0.01

data:
  max_seq_len: 20
  min_freq: 2
